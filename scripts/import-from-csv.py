#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –∏–º–ø–æ—Ä—Ç–∞ —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ CSV —Å –∑–∞–≥—Ä—É–∑–∫–æ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
"""
import csv
import os
import json
import re
import sys
from pathlib import Path
from urllib.request import urlopen, Request
from urllib.error import HTTPError, URLError
import time

# –ü—É—Ç–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –ø—É—Ç–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ
PROJECT_ROOT = Path('/vercel/share/v0-project')
CSV_FILE = PROJECT_ROOT / 'scripts' / 'products.csv'
IMAGES_DIR = PROJECT_ROOT / 'public' / 'images' / 'products'
LIB_DIR = PROJECT_ROOT / 'lib'
OUTPUT_FILE = LIB_DIR / 'imported-products.ts'

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ CSV
if not CSV_FILE.exists():
    print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {CSV_FILE}")
    print(f"–¢–µ–∫—É—â–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}")
    print(f"–°–æ–¥–µ—Ä–∂–∏–º–æ–µ scripts/:")
    scripts_dir = PROJECT_ROOT / 'scripts'
    if scripts_dir.exists():
        for f in scripts_dir.iterdir():
            print(f"  - {f.name}")
    sys.exit(1)

# –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
IMAGES_DIR.mkdir(parents=True, exist_ok=True)
LIB_DIR.mkdir(parents=True, exist_ok=True)

def clean_text(text):
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤"""
    if not text:
        return ''
    return text.strip()

def download_image(url, filename):
    """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å URL"""
    if not url or url == '':
        return None
    
    try:
        # –°–æ–∑–¥–∞—ë–º –∑–∞–ø—Ä–æ—Å —Å User-Agent
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        req = Request(url, headers=headers)
        
        with urlopen(req, timeout=30) as response:
            data = response.read()
            
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∏–∑ URL
        ext = '.jpg'
        if '.png' in url.lower():
            ext = '.png'
        elif '.jpeg' in url.lower():
            ext = '.jpeg'
            
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
        file_path = IMAGES_DIR / f"{filename}{ext}"
        with open(file_path, 'wb') as f:
            f.write(data)
            
        return f"/images/products/{filename}{ext}"
    except (HTTPError, URLError) as e:
        print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
        return None
    except Exception as e:
        print(f"  ‚ö†Ô∏è  –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {e}")
        return None

def slugify(text):
    """–°–æ–∑–¥–∞–Ω–∏–µ slug –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    text = text.lower()
    text = re.sub(r'[^\w\s-]', '', text)
    text = re.sub(r'[-\s]+', '-', text)
    return text.strip('-')

def parse_price(text):
    """–ü–∞—Ä—Å–∏–Ω–≥ —Ü–µ–Ω—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    if not text:
        return 0
    try:
        # –£–±–∏—Ä–∞–µ–º –≤—Å–µ –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä –∏ —Ç–æ—á–∫–∏/–∑–∞–ø—è—Ç–æ–π
        cleaned = re.sub(r'[^\d.,]', '', str(text))
        cleaned = cleaned.replace(',', '.')
        return float(cleaned) if cleaned else 0
    except:
        return 0

def main():
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∏–º–ø–æ—Ä—Ç —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ CSV...")
    
    if not CSV_FILE.exists():
        print(f"‚ùå CSV —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {CSV_FILE}")
        print("\n–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ CSV —Ñ–∞–π–ª –∫–∞–∫ scripts/products.csv")
        return
    
    products = []
    
    # –ß–∏—Ç–∞–µ–º CSV
    print(f"\nüìñ –ß–∏—Ç–∞–µ–º CSV —Ñ–∞–π–ª: {CSV_FILE}")
    with open(CSV_FILE, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        rows = list(reader)
    
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(rows)} —Å—Ç—Ä–æ–∫ –≤ CSV")
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ç–æ–≤–∞—Ä
    for idx, row in enumerate(rows, 1):
        article = clean_text(row.get('–ê—Ä—Ç–∏–∫—É–ª', ''))
        name = clean_text(row.get('–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –¥–ª—è —Å–∞–π—Ç–∞', ''))
        
        if not article or not name:
            continue
            
        print(f"\n[{idx}/{len(rows)}] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {name}")
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        product = {
            'id': article.replace('/', '-').replace('\\', '-'),
            'name': name,
            'article': article,
            'collection': clean_text(row.get('–ö–æ–ª–ª–µ–∫—Ü–∏—è', '')),
            'brand': clean_text(row.get('–ë—Ä–µ–Ω–¥', 'Cersanit')),
            'product_type': clean_text(row.get('–¢–∏–ø —ç–ª–µ–º–µ–Ω—Ç–∞', '')),
            'color': clean_text(row.get('–¶–≤–µ—Ç –ø–ª–∏—Ç–∫–∏', '')),
            'format': clean_text(row.get('–§–æ—Ä–º–∞—Ç –ø–ª–∏—Ç—ã –Ω–æ–º–∏–Ω–∞–ª—å–Ω—ã–π', '')),
            'surface': clean_text(row.get('–ü–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å', '')),
            'material': clean_text(row.get('–ú–∞—Ç–µ—Ä–∏–∞–ª', '')),
            'application': clean_text(row.get('–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ', '')),
            'price_retail': 0,  # –¶–µ–Ω—ã –Ω–µ—Ç –≤ CSV
            'price_wholesale': 0,
            'in_stock': True,
            'images': []
        }
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–æ—Ç–æ –ø–ª–∏—Ç—ã
        photo_url = clean_text(row.get('–§–æ—Ç–æ –ø–ª–∏—Ç—ã', ''))
        if photo_url:
            print(f"  üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–ª–∞–≤–Ω–æ–µ —Ñ–æ—Ç–æ...")
            image_filename = f"{slugify(article)}-main"
            image_path = download_image(photo_url, image_filename)
            if image_path:
                product['images'].append(image_path)
                product['image'] = image_path
                print(f"  ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {image_path}")
            time.sleep(0.5)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–æ—Ç–æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        collection_photo = clean_text(row.get('–§–æ—Ç–æ –ö–æ–ª–ª–µ–∫—Ü–∏–∏', ''))
        if collection_photo and collection_photo not in [photo_url]:
            print(f"  üì• –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–æ—Ç–æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏...")
            image_filename = f"{slugify(article)}-collection"
            image_path = download_image(collection_photo, image_filename)
            if image_path:
                product['images'].append(image_path)
                print(f"  ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {image_path}")
            time.sleep(0.5)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–æ—Ç–æ
        extra_photos = clean_text(row.get('–î–æ–ø —Ñ–æ—Ç–æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏', ''))
        if extra_photos:
            extra_urls = [url.strip() for url in extra_photos.split(';') if url.strip()]
            for i, extra_url in enumerate(extra_urls[:3], 1):  # –ú–∞–∫—Å 3 –¥–æ–ø —Ñ–æ—Ç–æ
                print(f"  üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–ø —Ñ–æ—Ç–æ {i}...")
                image_filename = f"{slugify(article)}-extra-{i}"
                image_path = download_image(extra_url, image_filename)
                if image_path:
                    product['images'].append(image_path)
                    print(f"  ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {image_path}")
                time.sleep(0.5)
        
        # –ï—Å–ª–∏ –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –¥–æ–±–∞–≤–ª—è–µ–º –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä
        if not product['images']:
            product['image'] = '/images/placeholder.jpg'
            product['images'] = ['/images/placeholder.jpg']
        elif 'image' not in product:
            product['image'] = product['images'][0]
        
        products.append(product)
        print(f"  ‚úÖ –¢–æ–≤–∞—Ä –¥–æ–±–∞–≤–ª–µ–Ω ({len(product['images'])} —Ñ–æ—Ç–æ)")
    
    print(f"\n\nüì¶ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(products)}")
    print(f"üñºÔ∏è  –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {IMAGES_DIR}")
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º TypeScript —Ñ–∞–π–ª
    print(f"\nüìù –°–æ–∑–¥–∞—ë–º TypeScript —Ñ–∞–π–ª: {OUTPUT_FILE}")
    
    ts_content = '''// –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–ª —Å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Ç–æ–≤–∞—Ä–∞–º–∏
// –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: ''' + time.strftime('%Y-%m-%d %H:%M:%S') + '''

export interface ImportedProduct {
  id: string;
  name: string;
  article: string;
  collection: string;
  brand: string;
  product_type: string;
  color: string;
  format: string;
  surface: string;
  material: string;
  application: string;
  price_retail: number;
  price_wholesale: number;
  in_stock: boolean;
  image: string;
  images: string[];
}

export const importedProducts: ImportedProduct[] = '''
    
    ts_content += json.dumps(products, ensure_ascii=False, indent=2)
    ts_content += ';\n'
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write(ts_content)
    
    print(f"‚úÖ –§–∞–π–ª —Å–æ–∑–¥–∞–Ω: {OUTPUT_FILE}")
    print(f"\nüéâ –ò–º–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ!")
    print(f"   üìä –í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(products)}")
    print(f"   üìÅ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {IMAGES_DIR}")
    print(f"   üíæ –î–∞–Ω–Ω—ã–µ: {OUTPUT_FILE}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è–º
    collections = {}
    for p in products:
        coll = p['collection']
        collections[coll] = collections.get(coll, 0) + 1
    
    print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è–º:")
    for coll, count in sorted(collections.items(), key=lambda x: -x[1])[:10]:
        print(f"   {coll}: {count} —Ç–æ–≤–∞—Ä–æ–≤")

if __name__ == '__main__':
    main()
